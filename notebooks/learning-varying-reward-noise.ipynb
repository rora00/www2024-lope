{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e011ea3d-a0cf-41ba-95ad-e080e90b3d13",
   "metadata": {},
   "source": [
    "# Long-term OPL Synthetic Simulation: Varying sparsity factors\n",
    "\n",
    "In this notebook, we compare the long-term effectiveness of OPL methods with varying noise levels on the long-term reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c43938ef-2195-44d3-af11-33f0cd24b407",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T00:25:42.292017Z",
     "iopub.status.busy": "2023-08-18T00:25:42.291961Z",
     "iopub.status.idle": "2023-08-18T00:25:59.219677Z",
     "shell.execute_reply": "2023-08-18T00:25:59.219558Z",
     "shell.execute_reply.started": "2023-08-18T00:25:42.291998Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "from dataset import SyntheticDataset\n",
    "from learning import run_all_policy_learning_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d28cbf24-7100-4e34-a950-cdf32348cefc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-16T15:25:44.251301Z",
     "iopub.status.busy": "2023-08-16T15:25:44.251212Z",
     "iopub.status.idle": "2023-08-16T15:25:44.483003Z",
     "shell.execute_reply": "2023-08-16T15:25:44.482878Z",
     "shell.execute_reply.started": "2023-08-16T15:25:44.251279Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.set_theme(style='white')\n",
    "legend = [\"Reg-based\", \"IPS-PG\", \"DR-PG\", \"Long-term OPL (Ours)\"]\n",
    "palette = ['#FF6437', '#509BF5', '#F037A5', '#4B917D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c101df2-6c50-4551-9db3-fa7e80dc0912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e85bc40e-0f52-4218-a2f8-f207d5a8dc86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-17T02:48:31.661905Z",
     "iopub.status.busy": "2023-08-17T02:48:31.661784Z",
     "iopub.status.idle": "2023-08-17T02:48:31.848565Z",
     "shell.execute_reply": "2023-08-17T02:48:31.848436Z",
     "shell.execute_reply.started": "2023-08-17T02:48:31.661882Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### experiment configurations ###\n",
    "n_sims = 200 # number of simulation runs\n",
    "n_train_data = 1000 # sample size of the historical and short-term experiment data\n",
    "n_test_data = 1000 # number of samples to approximate the ground-truth performance of the policies\n",
    "n_actions = 30 # number of the actions\n",
    "x_dim = 5 # feature dimension\n",
    "a_dim = 5 # action feature dimension\n",
    "s_dim = 3 # short-term reward dimension (number of short-term metrics)\n",
    "lambda_ = 0.5 # contribution of the short-term rewards in the expected long-term reward function\n",
    "reward_type = \"continuous\" # binary or continuous\n",
    "beta = 1.0 # baseline policy\n",
    "\n",
    "reward_std_list = [1.0, 3.0, 5.0, 7.0, 9.0] # experiment parameter range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58fa5c93-b658-4ca3-a4eb-1bf8c8f7404f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-17T02:48:34.872074Z",
     "iopub.status.busy": "2023-08-17T02:48:34.871959Z",
     "iopub.status.idle": "2023-08-17T12:46:21.554389Z",
     "shell.execute_reply": "2023-08-17T12:46:21.554234Z",
     "shell.execute_reply.started": "2023-08-17T02:48:34.872035Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_value_of_baseline_policy: 0.795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reward_std=1.0...: 100%|██████████| 200/200 [1:04:28<00:00, 19.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_value_of_baseline_policy: 0.809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reward_std=3.0...: 100%|██████████| 200/200 [1:03:34<00:00, 19.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_value_of_baseline_policy: 0.806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reward_std=5.0...: 100%|██████████| 200/200 [1:02:51<00:00, 18.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_value_of_baseline_policy: 0.786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reward_std=7.0...: 100%|██████████| 200/200 [1:03:09<00:00, 18.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_value_of_baseline_policy: 0.817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reward_std=9.0...: 100%|██████████| 200/200 [1:03:41<00:00, 19.11s/it]\n"
     ]
    }
   ],
   "source": [
    "result_df_list = []\n",
    "for reward_std in reward_std_list:\n",
    "    dataset = SyntheticDataset(\n",
    "        n_actions=n_actions, x_dim=x_dim, a_dim=a_dim, s_dim=s_dim, lambda_=lambda_,\n",
    "        reward_type=reward_type, reward_std=reward_std,\n",
    "    )\n",
    "    D_test = dataset.generate_dataset(n_data=n_test_data, beta=beta, baseline=True)\n",
    "    true_value_of_baseline_policy = dataset.calc_policy_value_beta(beta=beta)\n",
    "    print(f\"true_value_of_baseline_policy: {np.round(true_value_of_baseline_policy, 3)}\")\n",
    "\n",
    "    test_policy_value_list = []\n",
    "    for _ in tqdm(range(n_sims), desc=f\"reward_std={reward_std}...\"):\n",
    "        D_H = dataset.generate_dataset(n_data=n_train_data, beta=beta) # historical data generated by a baseline policy\n",
    "        D_E_0 = dataset.generate_dataset(n_data=n_train_data, beta=beta, baseline=True) # short-term experiment data generated by a baseline policy\n",
    "\n",
    "        true_value_of_learned_policies = run_all_policy_learning_methods(D_H, D_E_0, D_test)\n",
    "        test_policy_value_list.append(true_value_of_learned_policies)\n",
    "\n",
    "    ## summarize results\n",
    "    result_df = (\n",
    "        DataFrame(DataFrame(test_policy_value_list).stack())\n",
    "        .reset_index(1)\n",
    "        .rename(columns={\"level_1\": \"method\", 0: \"value\"})\n",
    "    )\n",
    "    result_df[\"reward_std\"] = reward_std\n",
    "    result_df[\"log_policy_value\"] = true_value_of_baseline_policy\n",
    "    result_df[\"rel_value\"] = result_df[\"value\"] / true_value_of_baseline_policy\n",
    "    result_df_list.append(result_df)\n",
    "result_df = pd.concat(result_df_list).reset_index(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b521fe9-4ed8-438d-b879-60e11adf827c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
