{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e011ea3d-a0cf-41ba-95ad-e080e90b3d13",
   "metadata": {},
   "source": [
    "# Long-term OPL Synthetic Simulation: Varying data sizes\n",
    "\n",
    "In this notebook, we compare the long-term effectiveness of OPL methods with varying data sizes of historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c43938ef-2195-44d3-af11-33f0cd24b407",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-16T15:26:03.283351Z",
     "iopub.status.busy": "2023-08-16T15:26:03.283307Z",
     "iopub.status.idle": "2023-08-16T15:26:08.133030Z",
     "shell.execute_reply": "2023-08-16T15:26:08.131880Z",
     "shell.execute_reply.started": "2023-08-16T15:26:03.283334Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "from dataset import SyntheticDataset\n",
    "from learning import run_all_policy_learning_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d28cbf24-7100-4e34-a950-cdf32348cefc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-16T15:28:08.865741Z",
     "iopub.status.busy": "2023-08-16T15:28:08.865614Z",
     "iopub.status.idle": "2023-08-16T15:28:09.184443Z",
     "shell.execute_reply": "2023-08-16T15:28:09.184318Z",
     "shell.execute_reply.started": "2023-08-16T15:28:08.865720Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.set_theme(style='white')\n",
    "legend = [\"Reg-based\", \"IPS-PG\", \"DR-PG\", \"Long-term OPL (Ours)\"]\n",
    "palette = ['#FF6437', '#509BF5', '#F037A5', '#4B917D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c101df2-6c50-4551-9db3-fa7e80dc0912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e85bc40e-0f52-4218-a2f8-f207d5a8dc86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-17T02:48:40.684750Z",
     "iopub.status.busy": "2023-08-17T02:48:40.683670Z",
     "iopub.status.idle": "2023-08-17T02:48:40.363007Z",
     "shell.execute_reply": "2023-08-17T02:48:40.362867Z",
     "shell.execute_reply.started": "2023-08-17T02:48:40.68451Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### experiment configurations ###\n",
    "n_sims = 200 # number of simulation runs\n",
    "n_test_data = 1000 # number of samples to approximate the ground-truth performance of the policies\n",
    "n_actions = 30 # number of the actions\n",
    "x_dim = 5 # feature dimension\n",
    "a_dim = 5 # action feature dimension\n",
    "s_dim = 3 # short-term reward dimension (number of short-term metrics)\n",
    "lambda_ = 0.5 # contribution of the short-term rewards in the expected long-term reward function\n",
    "reward_std = 7.0 # level of noise on the long-term reward\n",
    "reward_type = \"continuous\" # binary or continuous\n",
    "beta = 1.0 # baseline policy\n",
    "\n",
    "n_train_data_list = [500, 1000, 2000, 4000] # experiment parameter range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58fa5c93-b658-4ca3-a4eb-1bf8c8f7404f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-17T02:48:41.760776Z",
     "iopub.status.busy": "2023-08-17T02:48:41.760689Z",
     "iopub.status.idle": "2023-08-17T13:56:14.629725Z",
     "shell.execute_reply": "2023-08-17T13:56:14.629552Z",
     "shell.execute_reply.started": "2023-08-17T02:48:41.760756Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_value_of_baseline_policy: 0.791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "n_train_data=500...:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "n_train_data=500...: 100%|██████████| 200/200 [34:00<00:00, 10.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_value_of_baseline_policy: 0.814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "n_train_data=1000...: 100%|██████████| 200/200 [1:05:28<00:00, 19.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_value_of_baseline_policy: 0.796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "n_train_data=2000...: 100%|██████████| 200/200 [2:07:59<00:00, 38.40s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_value_of_baseline_policy: 0.798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "n_train_data=4000...: 100%|██████████| 200/200 [4:09:29<00:00, 74.85s/it]  \n"
     ]
    }
   ],
   "source": [
    "result_df_list = []\n",
    "for n_train_data in n_train_data_list:\n",
    "    dataset = SyntheticDataset(\n",
    "        n_actions=n_actions, x_dim=x_dim, a_dim=a_dim, s_dim=s_dim, lambda_=lambda_,\n",
    "        reward_type=reward_type, reward_std=reward_std,\n",
    "    )\n",
    "    D_test = dataset.generate_dataset(n_data=n_test_data, beta=beta, baseline=True)\n",
    "    true_value_of_baseline_policy = dataset.calc_policy_value_beta(beta=beta)\n",
    "    print(f\"true_value_of_baseline_policy: {np.round(true_value_of_baseline_policy, 3)}\")\n",
    "\n",
    "    test_policy_value_list = []\n",
    "    for _ in tqdm(range(n_sims), desc=f\"n_train_data={n_train_data}...\"):\n",
    "        D_H = dataset.generate_dataset(n_data=n_train_data, beta=beta) # historical data generated by a baseline policy\n",
    "        D_E_0 = dataset.generate_dataset(n_data=n_train_data, beta=beta, baseline=True) # short-term experiment data generated by a baseline policy\n",
    "\n",
    "        true_value_of_learned_policies = run_all_policy_learning_methods(D_H, D_E_0, D_test)\n",
    "        test_policy_value_list.append(true_value_of_learned_policies)\n",
    "\n",
    "    ## summarize results\n",
    "    result_df = (\n",
    "        DataFrame(test_policy_value_list).stack().reset_index(1)\n",
    "        .rename(columns={\"level_1\": \"method\", 0: \"value\"})\n",
    "    )\n",
    "    result_df[\"n_train_data\"] = n_train_data\n",
    "    result_df[\"log_policy_value\"] = true_value_of_baseline_policy\n",
    "    result_df[\"rel_value\"] = result_df[\"value\"] / true_value_of_baseline_policy\n",
    "    result_df_list.append(result_df)\n",
    "result_df = pd.concat(result_df_list).reset_index(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b521fe9-4ed8-438d-b879-60e11adf827c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
